[
["start-up-notes.html", "splithalf_documentation 1 Start-up notes 1.1 Instalation 1.2 Version note 1.3 Citing the package 1.4 User feedback 1.5 A note on terminology used in this document", " splithalf_documentation Sam Parsons 2018-11-13 1 Start-up notes 1.1 Instalation The splithalf package can be installed from CRAN or Github: install.packages(&quot;splithalf&quot;) # or the development version # devtools::install_github(&quot;sdparsons/splithalf&quot;) also note that the package requires the following; tidyr, dplyr, Rcpp, robustbase. You can load them with the following library(&quot;tidyr&quot;) library(&quot;dplyr&quot;) library(&quot;Rcpp&quot;) library(&quot;robustbase&quot;) 1.2 Version note The current version of splithalf is 0.5.1 [unofficial version name: Fight Milk] A huge part of developing this latest version has been condensing the many splithalf functions into a single one. In addition to this, I have adapted the underlying code using the amazing Rcpp package (Eddelbuettel et al. 2018), enabling me to input some C++ code. While my C++ code is rudimentary, the splithalf function now runs around 20x faster than before. 1.3 Citing the package Citing packages is one way for developers to gain some recognition for the time spent maintaining the work. I would like to keep track of how the package is used so that I can solicit feedback and improve the package more generally. This would also help me track the uptake of reporting measurement reliability over time. Please use the following reference: Parsons, S. (2017). splithalf: robust estimates of split half reliability (Version 2). figshare. https://doi.org/10.6084/m9.figshare.5559175.v2 If (eventually) this documentation turns into a publication, this reference will change. 1.4 User feedback Developing the splithalf package is a labour of love (and, occasionally, burning hatred). If you have any suggestions for improvement, additional functionality, or anything else, please contact me or raise an issue on github. Likewise, if you are having trouble using the package (e.g. scream fits at your computer screen – we’ve all been there) do contact me and I will do my best to help as quickly as possible. These kind of help requests are super welcome. In fact, the package has seen several increases in performance and usability due to people asking for help. 1.5 A note on terminology used in this document It is important that we have a similar understanding of the terminology I use in the package and documentation. Each is also discussed in reference to the functions later in the documentation. Trial – whatever happens in this task, e.g. a stimuli is presented. Importantly, participants give one response per trial Trial type – often trials can be split into different trial types (e.g. to compare congruent and incongruent trials) Condition - this might be different blocks of trials, or something to be assessed separately within the functions. e.g. a task might have a block of ‘positive’ trials and a block of ‘negative’ trials. Datatype - I use this to refer to the outcome of interest. specifically whether one is interested in average response times or accuracy rates Score - I use score to indicate how the final outcome is measured; e.g. the average RT, or the difference between two average RTs, or even the difference between two differences between two RTs (yes, the final one is confusing) References "],
["background.html", "2 Background 2.1 Why should I estimate the reliabilty of my task measurement? 2.2 Why did I develop this package in the first place? 2.3 How does splithalf work?", " 2 Background 2.1 Why should I estimate the reliabilty of my task measurement? 2.2 Why did I develop this package in the first place? Long story short-ish. I had some dot-probe attention bias data pre- and post- an attention bias modification procedure. In an exploratory analysis the post-training bias, but not the pre-training bias, was associated with self-report measures at follow-up. I think on a whim, we looked at how reliable the measures were pre/post and the post-training measure was much more reliable (~ .7) than the pre-training bias measure (~ .4). This led me down the road of wanting to understand reliability; what it ‘means’, how it impacts our results, the interesting reliability-power relationship, and so on. It also led me to the realisation (shared with many others), that we should estimate and report the reliability of our measures as standard practice. That has become my quest (albeit, it’s not what I am actualy paid to do), taking two main forms. I wrote a paper with my DPhil supervisors (Parsons, Kruijt, and Fox 2018), that I hope will be out in AMPPS soon (after revisions); the message being that we should adopt reporting reliabilty as a standard practice. I developed the splithalf package with the aim to provide an easy to use tool to estimate internal consistency of bias measures (difference scores) drawn from cognitive tasks. 2.3 How does splithalf work? The permutation approach in splithalf is actually rather simple. Over many repetitions (or permutations), the data is split in half and outcome scores are calculated for each half. For each repetion, the correlation coefficient between each half is calculated. Finally, the average of these correlations is taken as the final estimate of reliability. 95% percentiles are also taken from the distribution of estimates to give a picture of the spread of reliability estimates.1 References "],
["preprocessing.html", "3 Preprocessing", " 3 Preprocessing Splithalf requires that the input dataset has already undergone preprocessing (e.g. removal of error trials, RT trimming, and participants with high error rates). Splithalf should therefore be used with the same data that will be used to calculate summary scores and outcome indices. In my earlier attempts to make splithalf as useful as possible I added a number of user-inputted variables that helped remove participants and trim RTs. This also resulted in far to many input variables and potential confusion. You might need to do a little pre-processing if you have not saved your processing steps. Here is a code snippet that will work (each line includes a note about what the code is doing after the hash). You will need to change the numbers to match your data. This code could be briefer, however has been structured like this for ease of use. Note == indicates ‘is equal to’, :: indicates that the function uses the package indicated, in the first case the dplyr package (Wickham et al. 2018). dataset %&gt;% filter(accuracy == 1) %&gt;% # keeps only trials in which participants made an accurate response filter(RT &gt;= 100, RT &lt;= 2000) %&gt;% # removes RTs less than 100ms and greater than 2000ms filter(participant != c(“p23”, “p45”) # removes participants “p23” and “p45” If following rt trims you also trimmed by SD, use the following as well. Note that this is for two standard deviations from the mean, within each participant, and within each condition and trialtype. dataset %&gt;% dplyr::group_by(participant, condition, compare) %&gt;% dplyr::mutate(low = mean(RT) - (2 * sd(RT)), high = mean(RT) + (2 * sd(RT))) %&gt;% dplyr::filter(RT &gt;= low &amp; RT &lt;= high) If you want to save yourself effort in running splithalf, you could also rename your variable (column) names to the function defaults using the following dplyr::rename(dataset, RT = &quot;latency&quot;, condition = FALSE, participant = &quot;subject&quot;, correct = &quot;correct&quot;, trialnum = &quot;trialnum&quot;, compare = &quot;congruency&quot;) References "],
["examples.html", "4 Examples 4.1 Questions to ask before running splithalf 4.2 The example dataset 4.3 Difference scores 4.4 Average scores 4.5 Difference-of-difference scores", " 4 Examples This is what you are really here for, isn’t it? I am assuming that you have preprocessed your data to remove any outliers, use any cutoffs etc. These functions should be used on the data that you will then create your scores from, i.e. there should not be further processing to do before you run any of these. If there is, then these internal consistency reliability estimates will not reflect the reliability of the outcome measurements you actually analyse. 4.1 Questions to ask before running splithalf These questions should feed into what settings are appropriate for your need, and are aimed to make the splithalf function easy to use. What is the type of data you have? Are you interested in response times, or accuracy rates? Knowing this, you can set outcome = “RT”, or outcome = “accuracy” How is your outcome score calculated? Say that your response time based task has two trial types; “incongruent” and “congruent”. When you analyse your data will you use the average RT in each trial type, or will you create a difference score (or bias) by e.g. subtracting the average RT in congruent trials from the average RT in incongruent trials. The first I call “raw” and the Which method would you like to use to estimate (split-half) reliability? A super common way is to split the data into odd and even trials. Another is to split by the first half and second half of the trials. Both approaches are implemented in the splithalf funciton. However, I believe that the permutation splithalf approach is the way forward (and it was the reason why this package was developed, so please use it). 4.2 The example dataset For this quick example, we will simulate some data. Lets say we have 60 participants, who each complete a task with two blocks (A and B) of 80 trials. Trials are also evenly distributed between “congruent” and “incongruent” trials. For each trial we have RT data, and are assuming that participants were accurate in all trials. n_participants = 60 # sample size n_trials = 80 n_blocks = 2 sim_data &lt;- data.frame(participant_number = rep(1:n_participants, each = n_blocks * n_trials), trial_number = rep(1:n_trials, times = n_blocks * n_participants), block_name = rep(c(&quot;A&quot;,&quot;B&quot;), each = n_trials, length.out = n_participants * n_trials * n_blocks), trial_type = rep(c(&quot;congruent&quot;,&quot;incongruent&quot;), length.out = n_participants * n_trials * n_blocks), RT = rnorm(n_participants * n_trials * n_blocks, 500, 200)) 4.3 Difference scores This is by far the most common outcome measure I have come across, so lets start with that. Our data will be analysed so that we have two ‘bias’ or ‘difference score’ outcomes. So, within each block, we will take the average RT in congruent trials and subtract the average RT in incongruent trials. Calculating the final scores for each participant and for each block separately might look a bit like this Warning: package &#39;ggplot2&#39; was built under R version 3.4.4 Warning: package &#39;tidyr&#39; was built under R version 3.4.4 Warning: package &#39;purrr&#39; was built under R version 3.4.4 Warning: package &#39;dplyr&#39; was built under R version 3.4.4 Warning: package &#39;stringr&#39; was built under R version 3.4.4 Warning: package &#39;forcats&#39; was built under R version 3.4.4 Warning: package &#39;bindrcpp&#39; was built under R version 3.4.4 # A tibble: 120 x 5 # Groups: participant_number, block_name [120] participant_number block_name congruent incongruent bias &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 A 463. 513. -50.2 2 1 B 551. 480. 71.1 3 2 A 520. 521. -1.02 4 2 B 484. 520. -35.5 5 3 A 444. 518. -73.3 6 3 B 510. 550. -39.8 7 4 A 479. 519. -39.9 8 4 B 520. 580. -60.8 9 5 A 539. 485. 53.7 10 5 B 514. 509. 4.65 # ... with 110 more rows ok, lets see how reliable our A and B outcome scores (“bias”) are. library(&quot;splithalf&quot;) difference &lt;- splithalf(data = sim_data, outcome = &quot;RT&quot;, score = &quot;difference&quot;, conditionlist = c(&quot;A&quot;, &quot;B&quot;), halftype = &quot;random&quot;, permutations = 5000, var.RT = &quot;RT&quot;, var.condition = &quot;block_name&quot;, var.participant = &quot;participant_number&quot;, var.trialnum = &quot;trial_number&quot;, var.compare = &quot;trial_type&quot;, compare1 = &quot;congruent&quot;, compare2 = &quot;incongruent&quot;, average = &quot;mean&quot;) condition n splithalf 95_low 95_high spearmanbrown SB_low SB_high 1 A 60 -0.13 -0.31 0.07 -0.32 -0.89 0.13 2 B 60 -0.11 -0.29 0.08 -0.27 -0.80 0.16 4.3.1 Reading the output Unsurprisingly, our simlated random data does not yield internally consistant measurements. 4.3.2 Reporting the output Ideally, report everything. I have included 95% percentiles of the estimates to give a picture of the spread of internal consistency estimates. Also included is the spearman-brown corrected estimates, which take into account that the estimates are drawn from half the trials that they could have been. Negative reliabilities are near uninterpretable and the spearman-brown formula is not useful in this case. For comparibility between studies I recommend reporting both the raw and the corrected estimates. Something like the following should be sufficient. We estimated the internal consitency of bias A and B using a permutation-based splithalf approach (Parsons 2018) with 5000 random splits. The splithalf internal consistency of bias A was r = -0.13, 95%PI [-0.31,0.07] (Spearman-Brown corrected estimates were rSB = -0.32, 95%PI [-0.89,0.13]) 4.4 Average scores OK, lets change things up and look at average scores only. In this case, imagine that we have only a single trial type and we can then ignore the trial type option. We will want separate outcome scores for each block of trials, but this time it is simply the average RT in each block. Lets see how that looks within splithalf. Note that the main difference is that we have omitted the inputs about what trial types to ‘compare’, as this is irrelevant for the current task. average &lt;- splithalf(data = sim_data, outcome = &quot;RT&quot;, score = &quot;raw&quot;, conditionlist = c(&quot;A&quot;, &quot;B&quot;), halftype = &quot;random&quot;, permutations = 5000, var.RT = &quot;RT&quot;, var.condition = &quot;block_name&quot;, var.participant = &quot;participant_number&quot;, var.trialnum = &quot;trial_number&quot;, average = &quot;mean&quot;) condition n splithalf 95_low 95_high spearmanbrown SB_low SB_high 1 A 60 -0.09 -0.27 0.1 -0.23 -0.73 0.18 2 B 60 -0.10 -0.28 0.1 -0.25 -0.79 0.18 4.5 Difference-of-difference scores The difference of differences score is a bit more complex, and perhaps also less common. I programmed this aspect of the package initially because I had seen a few papers that used a change in bias score in their analysis, and I wondered “I wonder how reliable that is as an individual difference measure”. Be warned, difference scores are nearly always less reliable than raw averages, and it’s very likely that differences-of-differences will be the least reliable amongst the bunch. So, lets say our dependant/outcome variable in our task is the difference between bias observed in block A and B. So our outcome is calculated something like this. BiasA = incongruent_A - congruent_A BiasB = incongruent_B - congruent_B Outcome = BiasB - BiasA References "],
["important-considerations.html", "5 Important considerations 5.1 how many permutations? 5.2 how fast is splithalf?", " 5 Important considerations 5.1 how many permutations? 5.2 how fast is splithalf? "],
["wish-list.html", "6 Wish list", " 6 Wish list The biggest task on my wishlist was to combine the six or so functions that formed earlier versions of the splithalf package into a single, unified, function. This I have achieved, so in no particular order I have some things I still want to achieve with this package in future versions. If you have any more, please contact me and I will add them to my list adding other outcome measures / scores. For example, signal detection indices like d’. prettier output including plotting the distribution of split-half reliability estimates. I think that could be a cool tool to visualise the date multilevel approaches to estimating reliability - though this would deviate from splithalf somewhat "],
["references.html", "References", " References "]
]
