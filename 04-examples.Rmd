# Examples

This is what you are really here for, isn't it?

I am assuming that you have preprocessed your data to remove any outliers, use any cutoffs etc. These functions should be used on the data that you will then create your scores from, i.e. there should not be further processing to do before you run any of these. If there is, then these internal consistency reliability estimates will not reflect the reliability of the outcome measurements you actually analyse. 


## Questions to ask before running splithalf

These questions should feed into what settings are appropriate for your need, and are aimed to make the _splithalf_ function easy to use. 

1. **What is the type of data you have? **

Are you interested in response times, or accuracy rates?

Knowing this, you can set _outcome_ = "RT", or _outcome_ = "accuracy"


2. **How is your outcome score calculated?**

Say that your response time based task has two trial types; "incongruent" and "congruent". When you analyse your data will you use the average RT in each trial type, or will you create a difference score (or bias) by e.g. subtracting the average RT in congruent trials from the average RT in incongruent trials. The first I call "raw" and the 


3. **Which method would you like to use to estimate (split-half) reliability?**

A super common way is to split the data into odd and even trials. Another is to split by the first half and second half of the trials. Both approaches are implemented in the _splithalf_ funciton. However, I believe that the permutation splithalf approach is the way forward (and it was the reason why this package was developed, so please use it).


## The example dataset

For this quick example, we will simulate some data. Lets say we have 60 participants, who each complete a task with two blocks (A and B) of 80 trials. Trials are also evenly distributed between "congruent" and "incongruent" trials. For each trial we have RT data, and are assuming that participants were accurate in all trials. 

```{r}
n_participants = 60 # sample size
n_trials = 80
n_blocks = 2

sim_data <- data.frame(participant_number = rep(1:n_participants, each = n_blocks * n_trials),
                       trial_number = rep(1:n_trials, times = n_blocks * n_participants),
                       block_name = rep(c("A","B"), each = n_trials, length.out = n_participants * n_trials * n_blocks),
                       trial_type = rep(c("congruent","incongruent"), length.out = n_participants * n_trials * n_blocks),
                       RT = rnorm(n_participants * n_trials * n_blocks, 500, 200))


```


## Difference scores

This is by far the most common outcome measure I have come across, so lets start with that. 

Our data will be analysed so that we have two 'bias' or 'difference score' outcomes. So, within each block, we will take the average RT in congruent trials and subtract the average RT in incongruent trials. Calculating the final scores for each participant and for each block separately might look a bit like this

```{r comment = NA, echo = FALSE, message = FALSE}
library("tidyverse")

sim_data %>%
  group_by(participant_number, block_name, trial_type) %>%
  summarise(average = mean(RT)) %>%
  spread(trial_type, average) %>%
  mutate(bias = congruent - incongruent)

```


ok, lets see how reliable our A and B outcome scores ("bias") are.


```{r message = FALSE, comment = NA, results = 'hide'}
library("splithalf")

difference <- splithalf(data = sim_data,
                        outcome = "RT",
                        score = "difference",
                        conditionlist = c("A", "B"),
                        halftype = "random",
                        permutations = 5000,
                        var.RT = "RT",
                        var.condition = "block_name",
                        var.participant = "participant_number",
                        var.trialnum = "trial_number",
                        var.compare = "trial_type",
                        compare1 = "congruent",
                        compare2 = "incongruent",
                        average = "mean")


```

```{r echo = FALSE, comment = NA}
difference

```

### Reading the output

Unsurprisingly, our simlated random data does not yield internally consistant measurements. 

### Reporting the output

Ideally, report everything. I have included 95% percentiles of the estimates to give a picture of the spread of internal consistency estimates. Also included is the spearman-brown corrected estimates, which take into account that the estimates are drawn from half the trials that they could have been. Negative reliabilities are near uninterpretable and the spearman-brown formula is not useful in this case. For comparibility between studies I recommend reporting both the raw and the corrected estimates. Something like the following should be sufficient. 

We estimated the internal consitency of bias A and B using a permutation-based splithalf approach [@R-splithalf] with 5000 random splits. The splithalf internal consistency of bias A was r = `r difference[1,3]`, 95%PI [`r difference[1,4]`,`r difference[1,5]`] (Spearman-Brown corrected estimates were r~SB~ = `r difference[1,6]`, 95%PI [`r difference[1,7]`,`r difference[1,8]`])




## Average scores

OK, lets change things up and look at average scores only. In this case, imagine that we have only a single trial type and we can then ignore the trial type option. We will want separate outcome scores for each block of trials, but this time it is simply the average RT in each block. Lets see how that looks within _splithalf_. Note that the main difference is that we have omitted the inputs about what trial types to 'compare', as this is irrelevant for the current task.

```{r message = FALSE, comment = NA, results = 'hide'}
average <- splithalf(data = sim_data,
                     outcome = "RT",
                     score = "raw",
                     conditionlist = c("A", "B"),
                     halftype = "random",
                     permutations = 5000,
                     var.RT = "RT",
                     var.condition = "block_name",
                     var.participant = "participant_number",
                     var.trialnum = "trial_number",
                     average = "mean")

```

```{r echo = FALSE, comment = NA}
average

```




## Difference-of-difference scores

The difference of differences score is a bit more complex, and perhaps also less common. I programmed this aspect of the package initially because I had seen a few papers that used a change in bias score in their analysis, and I wondered "I wonder how reliable that is as an individual difference measure". Be warned, difference scores are nearly always less reliable than raw averages, and it's very likely that differences-of-differences will be the least reliable amongst the bunch. 

So, lets say our dependant/outcome variable in our task is the difference between bias observed in block A and B. So our outcome is calculated something like this. 

BiasA = incongruent_A - congruent_A

BiasB = incongruent_B - congruent_B

Outcome = BiasB - BiasA


